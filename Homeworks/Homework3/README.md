
**Все графики и результаты экспериментов представлены в соответствующих Jupyter-ноутбуках.**

---

## Задание 1: Эксперименты с глубиной сети (`homework_depth_experiments.py`)

В этом задании исследовалось влияние количества скрытых слоев на производительность модели.

### 1.1 Сравнение моделей разной глубины

Мы обучили пять моделей с различным количеством скрытых слоев (1, 2, 3, 5 и 7) на датасете MNIST, используя одинаковую ширину слоев (256 нейронов). Для каждой модели были проведены следующие анализы:

* **Сравнение точности:** Оценка точности на обучающей и тестовой выборках.
* **Визуализация кривых обучения:** Построение графиков потерь и точности с помощью функции `plot_training_history`.
* **Анализ времени обучения:** Измерение и анализ времени обучения для понимания влияния глубины сети на вычислительные затраты.

### 1.2 Анализ переобучения

Исследовалось влияние глубины сети на переобучение:

* **Выявление переобучения:** Построение графиков точности на обучающей и тестовой выборках по эпохам для выявления признаков переобучения.
* **Определение оптимальной глубины:** Определение оптимальной глубины сети на основе лучшей тестовой точности.
* **Сравнение с регуляризацией:** Добавление **Dropout** (с коэффициентом 0.2) и **BatchNorm** к моделям и сравнение их результатов с базовыми моделями без регуляризации.
* **Анализ начала переобучения:** Определение эпох, на которых начинается переобучение (когда тестовая точность перестает расти или начинает снижаться).

---

## Задание 2: Эксперименты с шириной сети (`homework_width_experiments.py`)

В этом задании изучалось влияние ширины скрытых слоев на производительность модели.

### 2.1 Сравнение моделей разной ширины

Были созданы четыре модели с одинаковой глубиной (3 скрытых слоя) и различной шириной слоев:

* **Узкие:** `[64, 32, 16]`
* **Средние:** `[256, 128, 64]`
* **Широкие:** `[1024, 512, 256]`
* **Очень широкие:** `[2048, 1024, 512]`

Для каждой модели:

* **Обучение и сравнение точности:** Обучение на датасете MNIST и сравнение точности на обучающей и тестовой выборках.
* **Измерение времени и параметров:** Измерение времени обучения и подсчет количества параметров с помощью функции `count_parameters`.
* **Визуализация кривых точности:** Построение графиков тестовой точности для сравнения производительности всех моделей.

### 2.2 Оптимизация архитектуры

Проведен **поиск по сетке (grid search)** для нахождения оптимальной архитектуры сети, тестируя различные схемы ширины слоев:

* **Сужающиеся** (contracting)
* **Расширяющиеся** (expanding)
* **Постоянные** (constant)
* **С узким местом** (bottleneck)

Примеры протестированных конфигураций ширины слоев: `[512, 256, 128]`, `[128, 256, 512]`, `[256, 256, 256]` и другие.

* **Сравнение по финальной точности:** Модели сравнивались по финальной тестовой точности для определения лучшей архитектуры.
* **Визуализация результатов:** Результаты визуализированы с помощью столбчатой диаграммы, которая лучше подходит для сравнения дискретных архитектур, чем тепловая карта.

---

## Задание 3: Эксперименты с регуляризацией (`homework_regularization_experiments.py`)

В этом задании исследовалось влияние различных техник регуляризации на производительность и стабильность обучения.

### 3.1 Сравнение техник регуляризации

Были созданы и обучены модели с одинаковой архитектурой (3 скрытых слоя по 256 нейронов) на датасете MNIST, применяя различные техники регуляризации:

* **Без регуляризации**
* **Только Dropout** с коэффициентами 0.1, 0.3 и 0.5
* **Только BatchNorm**
* **Комбинация Dropout (0.3) и BatchNorm**
* **L2-регуляризация** (через `weight decay` в оптимизаторе)

Для каждой модели:

* **Сравнение финальной точности:** Анализ финальной точности на тестовой выборке.
* **Анализ стабильности обучения:** Оценка колебаний потерь и точности на графиках для анализа стабильности обучения.
* **Визуализация распределения весов:** Построение гистограмм для визуализации распределения весов, чтобы понять, как регуляризация влияет на параметры модели.

### 3.2 Адаптивная регуляризация

Были реализованы адаптивные техники регуляризации:

* **Dropout с изменяющимся коэффициентом:** Линейно уменьшающийся от 0.5 до 0.1 в течение обучения.
* **BatchNorm с разными значениями momentum:** 0.1 и 0.9.
* **Комбинация адаптивного Dropout и BatchNorm** с `momentum` 0.5.

Для этих моделей:

* **Анализ влияния на слои:** Анализ влияния этих техник на разные слои сети путем визуализации распределения весов для каждого линейного слоя в комбинированной модели.
* **Визуализация кривых обучения:** Построение кривых обучения для всех вариантов для оценки их производительности и стабильности.

---

**Общие примечания:**

Во всех заданиях использовались предоставленные модули (`datasets.py`, `models.py`, `trainer.py`, `utils.py`) и датасет MNIST для обеспечения единообразия экспериментов. Результаты были визуализированы для наглядного анализа.

---
