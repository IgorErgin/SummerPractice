Все графики и результаты эксперементов приведены в блокнотах
Задание 1: Эксперименты с глубиной сети (homework_depth_experiments.py)

1.1 Сравнение моделей разной глубины:
Я создал и обучил пять моделей с разным количеством скрытых слоев (1, 2, 3, 5 и 7 слоев) на датасете MNIST, используя одинаковую ширину слоев (256 нейронов). Для каждой модели:
Я сравнил точность на обучающей и тестовой выборках.
Я визуализировал кривые обучения (потери и точность) с помощью функции plot_training_history.
Я измерил и проанализировал время обучения, чтобы понять, как глубина сети влияет на вычислительные затраты.
1.2 Анализ переобучения:
Я исследовал влияние глубины сети на переобучение:
Я построил графики точности на обучающей и тестовой выборках по эпохам, чтобы выявить признаки переобучения.
Я определил оптимальную глубину сети, основываясь на лучшей тестовой точности.
Я добавил Dropout (с коэффициентом 0.2) и BatchNorm к моделям и сравнил их результаты с базовыми моделями без регуляризации.
Я проанализировал, на каких эпохах начинается переобучение (когда тестовая точность перестает расти или начинает снижаться).
Задание 2: Эксперименты с шириной сети (homework_width_experiments.py)

2.1 Сравнение моделей разной ширины:
Я создал четыре модели с одинаковой глубиной (3 скрытых слоя) и разной шириной слоев: узкие ([64, 32, 16]), средние ([256, 128, 64]), широкие ([1024, 512, 256]) и очень широкие ([2048, 1024, 512]). Для каждой модели:
Я обучил модель на датасете MNIST и сравнил точность на обучающей и тестовой выборках.
Я измерил время обучения и подсчитал количество параметров с помощью функции count_parameters.
Я визуализировал кривые тестовой точности для всех моделей, чтобы сравнить их производительность.
2.2 Оптимизация архитектуры:
Я провел поиск по сетке (grid search), чтобы найти оптимальную архитектуру сети, тестируя различные схемы ширины слоев: сужающиеся (contracting), расширяющиеся (expanding), постоянные (constant) и с узким местом (bottleneck).
Я обучил модели с разными конфигурациями ширины слоев, например, [512, 256, 128], [128, 256, 512], [256, 256, 256] и т.д.
Я сравнил их по финальной тестовой точности и определил лучшую архитектуру.
Я визуализировал результаты с помощью столбчатой диаграммы, так как она лучше подходит для сравнения дискретных архитектур, чем тепловая карта.
Задание 3: Эксперименты с регуляризацией (homework_regularization_experiments.py)

3.1 Сравнение техник регуляризации:
Я создал и обучил модели с одинаковой архитектурой (3 скрытых слоя по 256 нейронов) на датасете MNIST, применяя разные техники регуляризации:
Без регуляризации.
Только Dropout с коэффициентами 0.1, 0.3 и 0.5.
Только BatchNorm.
Комбинация Dropout (0.3) и BatchNorm.
L2-регуляризация (через weight decay в оптимизаторе).
Для каждой модели:
Я сравнил финальную точность на тестовой выборке.
Я проанализировал стабильность обучения, оценивая колебания потерь и точности на графиках.
Я визуализировал распределение весов с помощью гистограмм, чтобы понять, как регуляризация влияет на параметры модели.
3.2 Адаптивная регуляризация:
Я реализовал адаптивные техники регуляризации:
Dropout с изменяющимся коэффициентом (линейно уменьшающимся от 0.5 до 0.1 в течение обучения).
BatchNorm с разными значениями momentum (0.1 и 0.9).
Комбинацию адаптивного Dropout и BatchNorm с momentum 0.5.
Я проанализировал влияние этих техник на разные слои сети, визуализировав распределение весов для каждого линейного слоя в комбинированной модели.
Я визуализировал кривые обучения для всех вариантов, чтобы оценить их производительность и стабильность.
В каждом задании я использовал предоставленные модули (datasets.py, models.py, trainer.py, utils.py) и датасет MNIST, чтобы обеспечить единообразие экспериментов, и визуализировал результаты для наглядного анализа.
