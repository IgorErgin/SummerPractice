{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce22d116-3d59-4c23-870c-4b107040e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "import os\n",
    "\n",
    "# Кастомный класс датасета\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file, target_column, column_types=None, scaler_type='minmax', ignore_columns=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.target_column = target_column\n",
    "        self.ignore_columns = ignore_columns if ignore_columns is not None else []\n",
    "        self.scaler_type = scaler_type\n",
    "\n",
    "        self.df = self.df.drop(columns=[col for col in self.ignore_columns if col in self.df.columns])\n",
    "\n",
    "        if target_column not in self.df.columns:\n",
    "            raise ValueError(f\"Целевая переменная '{target_column}' не найдена в CSV-файле\")\n",
    "\n",
    "        self.column_types = column_types if column_types is not None else self._infer_column_types()\n",
    "        self.features = self.df.drop(columns=[target_column])\n",
    "        self.target = self.df[target_column]\n",
    "        self._preprocess()\n",
    "\n",
    "    def _infer_column_types(self):\n",
    "        column_types = {}\n",
    "        for col in self.df.columns:\n",
    "            if col == self.target_column:\n",
    "                continue\n",
    "            if self.df[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
    "                unique_values = self.df[col].nunique()\n",
    "                if unique_values == 2:\n",
    "                    column_types[col] = 'binary'\n",
    "                else:\n",
    "                    column_types[col] = 'numeric'\n",
    "            else:\n",
    "                column_types[col] = 'categorical'\n",
    "        return column_types\n",
    "\n",
    "    def _preprocess(self):\n",
    "        # Обработка пропущенных значений в признаках\n",
    "        for col in self.features.columns:\n",
    "            if self.column_types.get(col, 'numeric') in ['numeric', 'binary']:\n",
    "                # Проверяем, что столбец можно преобразовать в числовой тип\n",
    "                try:\n",
    "                    self.features[col] = pd.to_numeric(self.features[col], errors='coerce')\n",
    "                    self.features[col] = self.features[col].fillna(self.features[col].median())\n",
    "                except:\n",
    "                    print(f\"Предупреждение: не удалось вычислить медиану для столбца '{col}'. Заполняю модой.\")\n",
    "                    self.features[col] = self.features[col].fillna(self.features[col].mode()[0])\n",
    "            else:\n",
    "                self.features[col] = self.features[col].fillna(self.features[col].mode()[0])\n",
    "\n",
    "        # Обработка пропущенных значений в целевой переменной\n",
    "        try:\n",
    "            self.target = pd.to_numeric(self.target, errors='coerce')\n",
    "            self.target = self.target.fillna(self.target.median())\n",
    "            self.is_target_categorical = False\n",
    "        except:\n",
    "            self.target = self.target.fillna(self.target.mode()[0])\n",
    "            self.is_target_categorical = True\n",
    "\n",
    "        self.scaler = MinMaxScaler() if self.scaler_type == 'minmax' else StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "        numeric_cols = [col for col in self.features.columns if self.column_types.get(col, 'numeric') == 'numeric']\n",
    "        categorical_cols = [col for col in self.features.columns if self.column_types.get(col, 'numeric') == 'categorical']\n",
    "        binary_cols = [col for col in self.features.columns if self.column_types.get(col, 'numeric') == 'binary']\n",
    "\n",
    "        if numeric_cols:\n",
    "            self.features[numeric_cols] = self.scaler.fit_transform(self.features[numeric_cols])\n",
    "\n",
    "        if categorical_cols:\n",
    "            encoded_cats = self.onehot_encoder.fit_transform(self.features[categorical_cols])\n",
    "            encoded_cat_cols = self.onehot_encoder.get_feature_names_out(categorical_cols)\n",
    "            self.features = self.features.drop(columns=categorical_cols)\n",
    "            self.features[encoded_cat_cols] = encoded_cats\n",
    "\n",
    "        for col in binary_cols:\n",
    "            self.features[col] = (self.features[col] == self.features[col].unique()[1]).astype(float)\n",
    "\n",
    "        if self.is_target_categorical:\n",
    "            self.target = self.label_encoder.fit_transform(self.target)\n",
    "\n",
    "        self.X = torch.tensor(self.features.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.target.values, dtype=torch.long if self.is_target_categorical else torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.features.columns.tolist()\n",
    "\n",
    "    def get_num_features(self):\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return len(self.label_encoder.classes_) if self.is_target_categorical else None\n",
    "\n",
    "# Определение функции mse\n",
    "def mse(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean().item()\n",
    "\n",
    "# Модель для регрессии и классификации\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, 1 if num_classes == 2 else num_classes)\n",
    "        self.sigmoid = nn.Sigmoid() if num_classes == 2 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "# Функции для метрик и визуализации\n",
    "def compute_regression_metrics(y_true, y_pred):\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    mse_val = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse_val)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse_val, rmse, r2\n",
    "\n",
    "def compute_classification_metrics(y_true, y_pred, y_pred_proba, num_classes):\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1] if num_classes == 2 else y_pred_proba, multi_class='ovr')\n",
    "    except ValueError:\n",
    "        roc_auc = float('nan')\n",
    "    return precision, recall, f1, roc_auc\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, num_classes, epoch, task='classification'):\n",
    "    cm = confusion_matrix(y_true.cpu().numpy(), y_pred.cpu().numpy(), labels=range(num_classes))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "                yticklabels=[f'Class {i}' for i in range(num_classes)])\n",
    "    plt.title(f'Confusion Matrix at Epoch {epoch} ({task})')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(f'confusion_matrix_{task}_epoch_{epoch}.png')\n",
    "    plt.close()\n",
    "\n",
    "def log_epoch(epoch, loss, **metrics):\n",
    "    msg = f\"Epoch {epoch}: loss={loss:.4f}\"\n",
    "    for k, v in metrics.items():\n",
    "        msg += f\", {k}={v:.4f}\"\n",
    "    print(msg)\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, criterion, optimizer, train_dataloader, val_dataloader, epochs, num_classes, task='classification'):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "        for i, (batch_X, batch_y) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            if task == 'regression':\n",
    "                outputs = outputs.squeeze(-1)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                acc = mse(outputs, batch_y)\n",
    "            elif task == 'classification' and num_classes == 2:\n",
    "                outputs = outputs.squeeze(-1)\n",
    "                loss = criterion(outputs, batch_y.float())\n",
    "                acc = ((outputs > 0.5).float() == batch_y).float().mean().item()\n",
    "            else:\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                acc = (torch.argmax(outputs, dim=1) == batch_y).float().mean().item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "        avg_acc = total_acc / (i + 1)\n",
    "\n",
    "        model.eval()\n",
    "        val_y_true = []\n",
    "        val_y_pred = []\n",
    "        val_y_pred_proba = []\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_X, batch_y) in enumerate(val_dataloader):\n",
    "                outputs = model(batch_X)\n",
    "                if task == 'regression':\n",
    "                    outputs = outputs.squeeze(-1)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    y_pred = outputs\n",
    "                    y_pred_proba = outputs\n",
    "                    acc = mse(outputs, batch_y)\n",
    "                elif task == 'classification' and num_classes == 2:\n",
    "                    outputs = outputs.squeeze(-1)\n",
    "                    loss = criterion(outputs, batch_y.float())\n",
    "                    y_pred = (outputs > 0.5).float()\n",
    "                    y_pred_proba = torch.sigmoid(outputs).unsqueeze(-1)\n",
    "                    acc = (y_pred == batch_y).float().mean().item()\n",
    "                else:\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    y_pred = torch.argmax(outputs, dim=1)\n",
    "                    y_pred_proba = torch.softmax(outputs, dim=1)\n",
    "                    acc = (y_pred == batch_y).float().mean().item()\n",
    "\n",
    "                val_y_true.append(batch_y)\n",
    "                val_y_pred.append(y_pred)\n",
    "                val_y_pred_proba.append(y_pred_proba)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                total_val_acc += acc\n",
    "\n",
    "        avg_val_loss = total_val_loss / (i + 1)\n",
    "        avg_val_acc = total_val_acc / (i + 1)\n",
    "\n",
    "        val_y_true = torch.cat(val_y_true)\n",
    "        val_y_pred = torch.cat(val_y_pred)\n",
    "        val_y_pred_proba = torch.cat(val_y_pred_proba)\n",
    "\n",
    "        if task == 'classification':\n",
    "            unique_classes = torch.unique(val_y_true).numel()\n",
    "            if unique_classes < num_classes:\n",
    "                print(f\"Предупреждение на эпохе {epoch}: в валидационной выборке только {unique_classes} из {num_classes} классов\")\n",
    "\n",
    "        if task == 'regression':\n",
    "            mse_val, rmse, r2 = compute_regression_metrics(val_y_true, val_y_pred)\n",
    "            metrics = {'mse': mse_val, 'rmse': rmse, 'r2': r2}\n",
    "        else:\n",
    "            precision, recall, f1, roc_auc = compute_classification_metrics(val_y_true, val_y_pred, y_pred_proba, num_classes)\n",
    "            metrics = {'precision': precision, 'recall': recall, 'f1_score': f1, 'roc_auc': roc_auc}\n",
    "            if epoch % 10 == 0:\n",
    "                plot_confusion_matrix(val_y_true, val_y_pred, num_classes, epoch, task)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            log_epoch(epoch, avg_loss, accuracy=avg_acc, validation_loss=avg_val_loss, validation_accuracy=avg_val_acc, **metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f233941-0065-4c3d-b5df-d4e7a250a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение линейной регрессии на Boston Housing Dataset\n",
      "Количество признаков: 13\n",
      "Имена признаков: ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
      "Размер тренировочного датасета: 404\n",
      "Размер валидационного датасета: 102\n",
      "Epoch 10: loss=64.9597, accuracy=64.9597, validation_loss=60.6057, validation_accuracy=60.6057, mse=71.4220, rmse=8.4512, r2=0.1987\n",
      "Epoch 20: loss=50.3488, accuracy=50.3488, validation_loss=47.0618, validation_accuracy=47.0618, mse=56.6790, rmse=7.5285, r2=0.3641\n",
      "Epoch 30: loss=44.9113, accuracy=44.9113, validation_loss=42.4440, validation_accuracy=42.4440, mse=50.9118, rmse=7.1352, r2=0.4288\n",
      "Epoch 40: loss=42.3955, accuracy=42.3955, validation_loss=39.7146, validation_accuracy=39.7146, mse=47.3298, rmse=6.8797, r2=0.4690\n",
      "Epoch 50: loss=40.3412, accuracy=40.3412, validation_loss=37.6679, validation_accuracy=37.6679, mse=44.5879, rmse=6.6774, r2=0.4997\n",
      "Epoch 60: loss=35.3990, accuracy=35.3990, validation_loss=35.8536, validation_accuracy=35.8536, mse=42.3611, rmse=6.5085, r2=0.5247\n",
      "Epoch 70: loss=34.1822, accuracy=34.1822, validation_loss=34.4252, validation_accuracy=34.4252, mse=40.4802, rmse=6.3624, r2=0.5458\n",
      "Epoch 80: loss=32.9955, accuracy=32.9955, validation_loss=33.1805, validation_accuracy=33.1805, mse=38.9701, rmse=6.2426, r2=0.5628\n",
      "Epoch 90: loss=31.5851, accuracy=31.5851, validation_loss=32.1834, validation_accuracy=32.1834, mse=37.6424, rmse=6.1353, r2=0.5777\n",
      "Epoch 100: loss=30.3724, accuracy=30.3724, validation_loss=31.3007, validation_accuracy=31.3007, mse=36.5369, rmse=6.0446, r2=0.5901\n",
      "Модель сохранена в linear_regression.pth\n",
      "\n",
      "Обучение логистической регрессии на Breast Cancer Wisconsin Dataset\n",
      "Пропущенные значения в датасете до обработки:\n",
      "id            0\n",
      "feature_1     0\n",
      "feature_2     0\n",
      "feature_3     0\n",
      "feature_4     0\n",
      "feature_5     0\n",
      "feature_6     0\n",
      "feature_7     0\n",
      "feature_8     0\n",
      "feature_9     0\n",
      "feature_10    0\n",
      "feature_11    0\n",
      "feature_12    0\n",
      "feature_13    0\n",
      "feature_14    0\n",
      "feature_15    0\n",
      "feature_16    0\n",
      "feature_17    0\n",
      "feature_18    0\n",
      "feature_19    0\n",
      "feature_20    0\n",
      "feature_21    0\n",
      "feature_22    0\n",
      "feature_23    0\n",
      "feature_24    0\n",
      "feature_25    0\n",
      "feature_26    0\n",
      "feature_27    0\n",
      "feature_28    0\n",
      "feature_29    0\n",
      "feature_30    0\n",
      "diagnosis     0\n",
      "dtype: int64\n",
      "Уникальные значения в столбце 'diagnosis': [0.1189  0.08902 0.08758 0.173   0.07678 0.1244  0.08368 0.1151  0.1072\n",
      " 0.2075  0.08452 0.1048  0.1023  0.06287 0.1431  0.1341  0.08216 0.1142\n",
      " 0.07615 0.07259 0.08183 0.07773 0.09946 0.07526 0.09564 0.1059  0.1275\n",
      " 0.07421 0.09876 0.07919 0.09782 0.1402  0.08482 0.1123  0.1233  0.08633\n",
      " 0.1014  0.06169 0.05504 0.1071  0.07146 0.09606 0.1038  0.1027  0.09618\n",
      " 0.09185 0.07409 0.1179  0.08301 0.06917 0.06563 0.08025 0.07408 0.07987\n",
      " 0.07873 0.07036 0.08294 0.1094  0.06289 0.09026 0.0802  0.07712 0.1132\n",
      " 0.0849  0.1031  0.08911 0.09211 0.06641 0.1175  0.0641  0.06589 0.1084\n",
      " 0.1339  0.103   0.07609 0.06387 0.07191 0.1108  0.09964 0.07918 0.08851\n",
      " 0.1016  0.1051  0.09203 0.07924 0.08579 0.06846 0.09288 0.09261 0.08473\n",
      " 0.07246 0.06828 0.06206 0.06603 0.08234 0.07376 0.08988 0.08756 0.09353\n",
      " 0.07397 0.09382 0.06878 0.07552 0.1405  0.09097 0.07185 0.09789 0.08832\n",
      " 0.08468 0.08486 0.1082  0.1017  0.08541 0.07722 0.1065  0.1252  0.06111\n",
      " 0.08523 0.08456 0.08009 0.08006 0.07628 0.07182 0.079   0.06541 0.07779\n",
      " 0.08465 0.09241 0.08019 0.07619 0.07071 0.0761  0.08067 0.07343 0.06765\n",
      " 0.07147 0.06784 0.08151 0.08158 0.08096 0.08118 0.06769 0.1036  0.09218\n",
      " 0.07683 0.07014 0.06435 0.1486  0.1259  0.06772 0.08132 0.07738 0.05972\n",
      " 0.07898 0.07685 0.06251 0.09223 0.09082 0.09187 0.06085 0.07699 0.07228\n",
      " 0.093   0.06428 0.06771 0.07371 0.101   0.07313 0.06164 0.07848 0.1162\n",
      " 0.09519 0.05843 0.07319 0.08082 0.1284  0.08631 0.07427 0.09772 0.07697\n",
      " 0.06938 0.07097 0.06576 0.06306 0.1446  0.06871 0.06559 0.1205  0.08701\n",
      " 0.06949 0.09333 0.06558 0.09221 0.1013  0.08174 0.07867 0.08762 0.1086\n",
      " 0.0875  0.0974  0.0738  0.06469 0.1076  0.07474 0.05865 0.07993 0.05525\n",
      " 0.06818 0.1026  0.08365 0.07809 0.08255 0.07568 0.08718 0.08177 0.08797\n",
      " 0.1064  0.07623 0.06072 0.08269 0.08362 0.09585 0.1243  0.09061 0.07087\n",
      " 0.07307 0.08328 0.08178 0.07617 0.08677 0.07127 0.07796 0.08496 0.0651\n",
      " 0.06783 0.1297  0.06321 0.07614 0.07748 0.07198 0.1178  0.08147 0.07849\n",
      " 0.06487 0.08113 0.0895  0.07957 0.1005  0.1191  0.1019  0.1204  0.07999\n",
      " 0.06515 0.07484 0.06829 0.0757  0.08218 0.07587 0.07024 0.07062 0.0612\n",
      " 0.08022 0.08858 0.08175 0.07948 0.06033 0.06386 0.05737 0.06263 0.06912\n",
      " 0.0972  0.06688 0.07787 0.1063  0.06431 0.09981 0.06915 0.07009 0.06994\n",
      " 0.08799 0.08472 0.09584 0.07007 0.06922 0.06794 0.06643 0.07676 0.06777\n",
      " 0.09929 0.07764 0.09469 0.07842 0.07638 0.06745 0.08385 0.07804 0.06192\n",
      " 0.0658  0.06958 0.05695 0.08253 0.07434 0.08116 0.06174 0.06037 0.08198\n",
      " 0.1055  0.05932 0.09702 0.05933 0.08553 0.1024  0.07961 0.06888 0.07083\n",
      " 0.07037 0.082   0.07953 0.09124 0.09166 0.06522 0.07418 0.07207 0.07599\n",
      " 0.1009  0.0987  0.07664 0.08764 0.09825 0.0908  0.07806 0.08488 0.08083\n",
      " 0.08187 0.08763 0.0759  0.06825 0.105   0.08815 0.09438 0.07018 0.07188\n",
      " 0.08317 0.07113 0.07431 0.08136 0.05521 0.06658 0.07238 0.07582 0.06735\n",
      " 0.07632 0.0747  0.06494 0.08574 0.09614 0.06766 0.08666 0.07055 0.07701\n",
      " 0.0896  0.12    0.07061 0.09638 0.1403  0.09215 0.07287 0.09349 0.1118\n",
      " 0.0732  0.06836 0.08824 0.06623 0.1043  0.07602 0.08865 0.1007  0.07081\n",
      " 0.06609 0.07686 0.07053 0.09158 0.08121 0.1198  0.07262 0.07247 0.07834\n",
      " 0.05974 0.07732 0.07012 0.08503 0.06896 0.07745 0.07881 0.09206 0.09251\n",
      " 0.06165 0.07351 0.08304 0.09464 0.07123 0.08284 0.09208 0.08839 0.08061\n",
      " 0.09646 0.07662 0.06025 0.1155  0.09359 0.08075 0.08314 0.06827 0.07735\n",
      " 0.07234 0.06911 0.0671  0.09532 0.07944 0.0681  0.06736 0.08225 0.08251\n",
      " 0.09075 0.07285 0.07463 0.07425 0.09952 0.06091 0.08194 0.0781  0.0733\n",
      " 0.07675 0.0722  0.06788 0.06291 0.07211 0.0906  0.06464 0.07863 0.06925\n",
      " 0.1249  0.07875 0.1224  0.0927  0.08524 0.06639 0.08273 0.06743 0.108\n",
      " 0.07802 0.07858 0.07698 0.0918  0.09136 0.07729 0.07603 0.09326 0.0696\n",
      " 0.08181 0.1034  0.06596 0.09009 0.08024 0.08203 0.05871 0.07625 0.07028\n",
      " 0.07429 0.06599 0.1033  0.07661 0.09445 0.08999 0.08549 0.1183  0.07538\n",
      " 0.07277 0.1364  0.1168  0.0723  0.1067  0.09879 0.06142 0.1109  0.07048\n",
      " 0.06954 0.08893 0.08557 0.08982 0.09671 0.07613 0.09031 0.09209 0.1049\n",
      " 0.08665 0.07592 0.07253 0.08052 0.07757 0.07782 0.08278 0.07569 0.08351\n",
      " 0.0997  0.09938 0.1066  0.08134 0.06956 0.06443 0.08492 0.06953 0.07399\n",
      " 0.09479 0.0792  0.07626 0.06592 0.08032 0.06484 0.07393 0.07242 0.08283\n",
      " 0.06742 0.06969 0.08004 0.08732 0.08321 0.05905 0.1409  0.09873 0.07115\n",
      " 0.06637 0.0782  0.124   0.07039]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 30)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreast_cancer.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Создаем датасет\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m classification_dataset \u001b[38;5;241m=\u001b[39m CSVDataset(\n\u001b[0;32m     76\u001b[0m     csv_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreast_cancer.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     77\u001b[0m     target_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Исправлено с 'id' на 'target'\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     column_types\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m31\u001b[39m)},\n\u001b[0;32m     79\u001b[0m     scaler_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     80\u001b[0m     ignore_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Разделяем на тренировочную и валидационную выборки\u001b[39;00m\n\u001b[0;32m     84\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(classification_dataset))\n",
      "Cell \u001b[1;32mIn[41], line 30\u001b[0m, in \u001b[0;36mCSVDataset.__init__\u001b[1;34m(self, csv_file, target_column, column_types, scaler_type, ignore_columns)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[target_column])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[target_column]\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess()\n",
      "Cell \u001b[1;32mIn[41], line 79\u001b[0m, in \u001b[0;36mCSVDataset._preprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m binary_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_types\u001b[38;5;241m.\u001b[39mget(col, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numeric_cols:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[numeric_cols] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[numeric_cols])\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m categorical_cols:\n\u001b[0;32m     82\u001b[0m     encoded_cats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monehot_encoder\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[categorical_cols])\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    489\u001b[0m     X,\n\u001b[0;32m    490\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_pass,\n\u001b[0;32m    491\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[0;32m    492\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m )\n\u001b[0;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1134\u001b[0m         )\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 30)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "\n",
    "# Основной код\n",
    "if __name__ == '__main__':\n",
    "    # 1. Регрессия: Boston Housing Dataset\n",
    "    print(\"Обучение линейной регрессии на Boston Housing Dataset\")\n",
    "    regression_dataset = CSVDataset(\n",
    "        csv_file='https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv',\n",
    "        target_column='medv',\n",
    "        column_types={'chas': 'binary'},\n",
    "        scaler_type='minmax',\n",
    "        ignore_columns=[]\n",
    "    )\n",
    "\n",
    "    print(f\"Количество признаков: {regression_dataset.get_num_features()}\")\n",
    "    print(f\"Имена признаков: {regression_dataset.get_feature_names()}\")\n",
    "\n",
    "    train_size = int(0.8 * len(regression_dataset))\n",
    "    val_size = len(regression_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(regression_dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    print(f\"Размер тренировочного датасета: {len(train_dataset)}\")\n",
    "    print(f\"Размер валидационного датасета: {len(val_dataset)}\")\n",
    "\n",
    "    regression_model = LinearRegression(in_features=regression_dataset.get_num_features())\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(regression_model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "    regression_model = train_model(\n",
    "        regression_model, criterion, optimizer, train_dataloader, val_dataloader, \n",
    "        epochs=100, num_classes=1, task='regression'\n",
    "    )\n",
    "\n",
    "    regression_model_path = 'linear_regression.pth'\n",
    "    torch.save(regression_model.state_dict(), regression_model_path)\n",
    "    print(f\"Модель сохранена в {regression_model_path}\")\n",
    "\n",
    "    # 2. Бинарная классификация: Breast Cancer Wisconsin Dataset\n",
    "    print(\"\\nОбучение логистической регрессии на Breast Cancer Wisconsin Dataset\")\n",
    "    \n",
    "    # Загружаем датасет\n",
    "    breast_cancer_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
    "    df = pd.read_csv(breast_cancer_url, header=None)\n",
    "    df.columns = ['id'] + [f'feature_{i}' for i in range(1, 31)] + ['diagnosis']\n",
    "    \n",
    "    # Проверяем пропущенные значения в исходных данных\n",
    "    print(\"Пропущенные значения в датасете до обработки:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    # Проверяем уникальные значения в столбце 'diagnosis'\n",
    "    print(\"Уникальные значения в столбце 'diagnosis':\", df['diagnosis'].unique())\n",
    "    \n",
    "    # Удаляем строки с некорректными значениями в 'diagnosis' (не 'M' или 'B')\n",
    "    df = df[df['diagnosis'].isin(['M', 'B'])]\n",
    "    \n",
    "    # Проверяем, что после фильтрации нет пропущенных значений\n",
    "    if df['diagnosis'].isna().any():\n",
    "        raise ValueError(\"Пропущенные значения в столбце 'diagnosis' после фильтрации\")\n",
    "    \n",
    "    # Преобразуем целевую переменную\n",
    "    df['target'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "    \n",
    "    # Проверяем, что все значения в 'target' корректны\n",
    "    if df['target'].isna().any():\n",
    "        raise ValueError(\"Пропущенные значения в целевой переменной после маппинга\")\n",
    "    \n",
    "    # Удаляем столбец 'diagnosis', так как теперь у нас есть 'target'\n",
    "    df = df.drop(columns=['diagnosis'])\n",
    "    \n",
    "    # Сохраняем датасет\n",
    "    df.to_csv('breast_cancer.csv', index=False)\n",
    "    \n",
    "    # Создаем датасет\n",
    "    classification_dataset = CSVDataset(\n",
    "        csv_file='breast_cancer.csv',\n",
    "        target_column='target',  # Исправлено с 'id' на 'target'\n",
    "        column_types={f'feature_{i}': 'numeric' for i in range(1, 31)},\n",
    "        scaler_type='minmax',\n",
    "        ignore_columns=['id']\n",
    "    )\n",
    "\n",
    "    # Разделяем на тренировочную и валидационную выборки\n",
    "    train_size = int(0.8 * len(classification_dataset))\n",
    "    val_size = len(classification_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(classification_dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    print(f\"Размер тренировочного датасета: {len(train_dataset)}\")\n",
    "    print(f\"Размер валидационного датасета: {len(val_dataset)}\")\n",
    "\n",
    "    # Модель логистической регрессии\n",
    "    classification_model = LogisticRegression(in_features=classification_dataset.get_num_features(), num_classes=2)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(classification_model.parameters(), lr=0.1)\n",
    "\n",
    "    # Обучаем модель\n",
    "    classification_model = train_model(\n",
    "        classification_model, criterion, optimizer, train_dataloader, val_dataloader, \n",
    "        epochs=100, num_classes=2, task='classification'\n",
    "    )\n",
    "\n",
    "    # Сохраняем модель\n",
    "    torch.save(classification_model.state_dict(), 'logistic_regression.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870c629-ec60-4910-b05f-4848535452e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
