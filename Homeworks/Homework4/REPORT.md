# Отчет по исследованию архитектур нейронных сетей

## Введение

Цель данного проекта — исследование и сравнение различных архитектур нейронных сетей на задачах классификации изображений с использованием датасетов MNIST и CIFAR-10. В ходе работы были реализованы и проанализированы полносвязные сети, сверточные нейронные сети (CNN), а также кастомные слои и Residual блоки. Основное внимание уделялось влиянию архитектурных решений на производительность, время обучения и стабильность моделей.

---

## Задание 1: Сравнение CNN и полносвязных сетей

### Выводы
- **MNIST**:
  - Полносвязная сеть показала приемлемую точность, но уступила CNN из-за отсутствия способности эффективно обрабатывать пространственные данные.
  - Простая CNN обеспечила более высокую точность благодаря локальным связям и меньшему количеству параметров.
  - CNN с Residual блоками еще больше улучшила результаты, позволив строить глубокие сети без деградации производительности.
- **CIFAR-10**:
  - Полносвязная сеть показала низкую точность и склонность к переобучению из-за высокой сложности изображений.
  - Простая CNN справилась лучше, но недостаточно для сложных данных.
  - CNN с Residual блоками значительно повысила точность и обобщающую способность за счет глубоких архитектур и регуляризации.

**Общий вывод**: CNN превосходят полносвязные сети в задачах классификации изображений благодаря инвариантности к сдвигам и локальной обработке данных. Residual блоки делают глубокие CNN более эффективными и устойчивыми.

### Картинки
- Вставьте графики кривых обучения (точность и потери) для каждой модели на MNIST и CIFAR-10: `![Кривые обучения MNIST](mnist_learning_curve.png)` и `![Кривые обучения CIFAR-10](cifar_learning_curve.png)` — после раздела "Выводы".
- Вставьте confusion matrix для моделей на CIFAR-10: `![Confusion Matrix CIFAR-10](cifar_confusion_matrix.png)` — в конце подраздела.

---

## Задание 2: Анализ архитектур CNN

### Выводы
- **Влияние размера ядра свертки**:
  - Ядра 3x3 оказались наиболее сбалансированными по точности и вычислительной нагрузке.
  - Большие ядра (5x5, 7x7) увеличили время обучения и иногда ухудшали результаты из-за избыточной детализации.
  - Комбинация 1x1 + 3x3 сверток сократила количество параметров, сохранив качество предсказаний.
- **Влияние глубины CNN**:
  - Неглубокие сети (2 слоя) были быстрыми, но недостаточно точными.
  - Глубокие сети (6+ слоев) показали лучшую производительность, но без Residual связей страдали от исчезающего градиента.
  - Residual связи позволили эффективно обучать глубокие архитектуры, улучшив точность.

**Общий вывод**: Ядра 3x3 и глубокие сети с Residual связями обеспечивают оптимальную производительность для классификации изображений.

### Картинки
- Вставьте визуализации активаций первого слоя для моделей с разными размерами ядер: `![Активации 3x3](activation_3x3.png)` и `![Активации 5x5](activation_5x5.png)` — после подраздела "Влияние размера ядра свертки".
- Вставьте feature maps для моделей с разной глубиной: `![Feature Maps 2 слоя](feature_maps_2layers.png)` и `![Feature Maps 6 слоев](feature_maps_6layers.png)` — после подраздела "Влияние глубины CNN".

---

## Задание 3: Кастомные слои и эксперименты

### Выводы
- **Кастомные слои**:
  - Реализация собственных слоев (свертка, attention, pooling) дала гибкость в настройке моделей под задачу.
  - Однако такие слои требуют точной реализации backward проходов, что усложняет разработку.
- **Эксперименты с Residual блоками**:
  - Базовый Residual блок улучшил обучение глубоких сетей, но увеличил число параметров.
  - Bottleneck Residual блок снизил вычислительную нагрузку, сохранив точность.
  - Wide Residual блок повысил стабильность за счет увеличения ширины слоев.

**Общий вывод**: Кастомные слои полезны для специфических задач, но требуют аккуратной реализации. Bottleneck Residual блоки — оптимальный выбор для глубоких сетей с ограниченными ресурсами.

### Картинки
- Вставьте графики сравнения производительности для моделей с разными Residual блоками: `![Производительность Residual блоков](residual_performance.png)` — после подраздела "Эксперименты с Residual блоками".
- Вставьте графики градиентов для анализа стабильности: `![Градиенты обучения](gradient_stability.png)` — в конце подраздела.

---

## Заключение

Исследование подтвердило, что CNN с Residual блоками — наиболее эффективная архитектура для классификации изображений, особенно на сложных датасетах. Кастомные слои добавляют гибкости, но требуют тщательной реализации. Оптимальная архитектура зависит от задачи и доступных ресурсов.