                         # Отчет по выполненным заданиям

## Задание 1: Сравнение CNN и полносвязных сетей

В этом задании я сравнил производительность полносвязных нейронных сетей и сверточных нейронных сетей (CNN) на двух датасетах: MNIST и CIFAR-10. Для реализации я использовал PyTorch — мощный фреймворк для глубокого обучения.

### Работа с MNIST
Я реализовал три модели:
- Полносвязную сеть с 3-4 слоями.
- Простую CNN с 2-3 сверточными слоями.
- CNN с Residual блоками для улучшения обучения глубоких сетей.

Все модели я обучил с одинаковыми гиперпараметрами: learning rate, batch size и количеством эпох. После обучения я измерил точность на обучающей и тестовой выборках, а также время обучения и инференса. Для анализа я построил кривые обучения с помощью `matplotlib` и подсчитал количество параметров в каждой модели. Датасет загружал через `torchvision`, а для оптимизации использовал `torch.optim`.

### Работа с CIFAR-10
На этом датасете я также реализовал три модели:
- Полносвязную сеть.
- CNN с Residual блоками.
- CNN с Residual блоками и добавленной регуляризацией.

Я обучил их с одинаковыми гиперпараметрами и сравнил точность, время обучения и степень переобучения. Для более глубокого анализа я построил confusion matrix с помощью `seaborn` и исследовал поведение градиентов в моделях. Прогресс обучения отслеживал с помощью `tqdm`.

### Выводы
CNN показали значительно лучшую производительность по сравнению с полносвязными сетями, особенно на сложном датасете CIFAR-10. Residual блоки помогли улучшить обучение более глубоких архитектур.

---

## Задание 2: Анализ архитектур CNN

В этом задании я исследовал влияние размера ядра свертки и глубины CNN на производительность моделей, используя датасет CIFAR-10. Все эксперименты проводил в PyTorch.

### Влияние размера ядра свертки
Я создал несколько моделей с разными размерами ядер:
- 3x3.
- 5x5.
- 7x7.
- Комбинация 1x1 + 3x3.

Чтобы сравнение было честным, я поддерживал одинаковое количество параметров в моделях. После обучения я сравнил точность и время обучения, а также проанализировал размеры рецептивных полей. Для визуализации активаций первого слоя использовал `matplotlib`.

### Влияние глубины CNN
Я реализовал модели с разным количеством сверточных слоев:
- 2 слоя.
- 4 слоя.
- 6+ слоев.
- CNN с Residual связями.

Я сравнил их точность и время обучения, а также исследовал проблемы исчезающих или взрывающихся градиентов. Для борьбы с градиентами применил `torch.nn.utils.clip_grad_norm_`. Эффективность Residual связей я оценил, визуализируя feature maps с помощью `matplotlib`.

### Выводы
Ядра размера 3x3 показали себя как оптимальный компромисс между производительностью и скоростью. Глубокие сети без Residual связей страдали от проблем с градиентами, но добавление таких связей значительно улучшило результаты.

---

## Задание 3: Кастомные слои и эксперименты

В этом задании я реализовал кастомные слои и провел эксперименты с различными вариантами Residual блоков, используя PyTorch и датасет CIFAR-10.

### Реализация кастомных слоев
Я разработал следующие кастомные слои:
- Сверточный слой с дополнительной логикой.
- Механизм внимания (attention).
- Кастомную функцию активации.
- Кастомный pooling слой.

Для каждого слоя я написал forward и backward проходы, используя `torch.autograd.Function` для корректной работы автодифференцирования. Слои протестировал на простых тензорах и сравнил их со стандартными аналогами из `torch.nn`.

### Эксперименты с Residual блоками
Я реализовал три типа Residual блоков:
- Базовый Residual блок.
- Bottleneck Residual блок.
- Wide Residual блок.

После обучения моделей я сравнил их производительность, количество параметров и стабильность обучения. Эксперименты проводил на CIFAR-10, а результаты визуализировал с помощью `matplotlib`.

### Выводы
Кастомные слои позволили гибко настраивать архитектуру под специфические задачи, хотя их реализация потребовала тщательной отладки backward проходов. Residual блоки, особенно Bottleneck, показали хорошую производительность при меньшем количестве параметров.

---

## Итог
Работа над заданиями дала мне ценный опыт в проектировании, обучении и анализе нейронных сетей. Я освоил создание кастомных слоев, изучил влияние архитектурных решений на производительность и укрепил навыки работы с PyTorch.